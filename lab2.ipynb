{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Hahua Mykyta DA-22 Machine Learning Lab#2",
   "id": "87daeeeea007db59"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "First of all lets download and read data from google drive urls, i uploaded them on google drive",
   "id": "d0332e64b1ed0f6b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T09:55:04.348847Z",
     "start_time": "2024-11-28T09:54:52.911914Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import gdown\n",
    "import numpy as np\n",
    "import math\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_url = 'https://drive.google.com/uc?id=1wCKXZWpYEysIFn1F74R9pTOt6aFasKTy'\n",
    "test_url = 'https://drive.google.com/uc?id=1gTuliILSkSw9JYSW_RhpxcnqKS3Q-PMM'\n",
    "\n",
    "train = 'application_train.csv'\n",
    "test = 'application_test.csv'\n",
    "\n",
    "if not os.path.exists(train):\n",
    "    print(f\"{train} not found. Downloading...\")\n",
    "    gdown.download(train_url, train, quiet=False)\n",
    "else:\n",
    "    print(f\"{train} already exists. Skipping download.\")\n",
    "\n",
    "if not os.path.exists(test):\n",
    "    print(f\"{test} not found. Downloading...\")\n",
    "    gdown.download(test_url, test, quiet=False)\n",
    "else:\n",
    "    print(f\"{test} already exists. Skipping download.\")\n",
    "\n",
    "df_train = pd.read_csv(train)\n",
    "df_test = pd.read_csv(test)"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "application_train.csv not found. Downloading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1wCKXZWpYEysIFn1F74R9pTOt6aFasKTy\n",
      "From (redirected): https://drive.google.com/uc?id=1wCKXZWpYEysIFn1F74R9pTOt6aFasKTy&confirm=t&uuid=4e28f51a-9c1f-4a58-85a0-24981542bee0\n",
      "To: /Users/nikitagagua/PycharmProjects/machine learning/application_train.csv\n",
      " 10%|â–ˆ         | 16.8M/166M [00:02<00:19, 7.58MB/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[354], line 23\u001B[0m\n\u001B[1;32m     21\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mexists(train):\n\u001B[1;32m     22\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtrain\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m not found. Downloading...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 23\u001B[0m     \u001B[43mgdown\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdownload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_url\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mquiet\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m     24\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     25\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtrain\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m already exists. Skipping download.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/PycharmProjects/machine learning/.venv/lib/python3.9/site-packages/gdown/download.py:368\u001B[0m, in \u001B[0;36mdownload\u001B[0;34m(url, output, quiet, proxy, speed, use_cookies, verify, id, fuzzy, resume, format, user_agent, log_messages)\u001B[0m\n\u001B[1;32m    366\u001B[0m     pbar \u001B[38;5;241m=\u001B[39m tqdm\u001B[38;5;241m.\u001B[39mtqdm(total\u001B[38;5;241m=\u001B[39mtotal, unit\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mB\u001B[39m\u001B[38;5;124m\"\u001B[39m, initial\u001B[38;5;241m=\u001B[39mstart_size, unit_scale\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m    367\u001B[0m t_start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[0;32m--> 368\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m chunk \u001B[38;5;129;01min\u001B[39;00m res\u001B[38;5;241m.\u001B[39miter_content(chunk_size\u001B[38;5;241m=\u001B[39mCHUNK_SIZE):\n\u001B[1;32m    369\u001B[0m     f\u001B[38;5;241m.\u001B[39mwrite(chunk)\n\u001B[1;32m    370\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m quiet:\n",
      "File \u001B[0;32m~/PycharmProjects/machine learning/.venv/lib/python3.9/site-packages/requests/models.py:820\u001B[0m, in \u001B[0;36mResponse.iter_content.<locals>.generate\u001B[0;34m()\u001B[0m\n\u001B[1;32m    818\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mraw, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstream\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m    819\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 820\u001B[0m         \u001B[38;5;28;01myield from\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mraw\u001B[38;5;241m.\u001B[39mstream(chunk_size, decode_content\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m    821\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m ProtocolError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    822\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m ChunkedEncodingError(e)\n",
      "File \u001B[0;32m~/PycharmProjects/machine learning/.venv/lib/python3.9/site-packages/urllib3/response.py:1060\u001B[0m, in \u001B[0;36mHTTPResponse.stream\u001B[0;34m(self, amt, decode_content)\u001B[0m\n\u001B[1;32m   1058\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1059\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_fp_closed(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fp) \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_decoded_buffer) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m-> 1060\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mamt\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mamt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdecode_content\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdecode_content\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1062\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m data:\n\u001B[1;32m   1063\u001B[0m             \u001B[38;5;28;01myield\u001B[39;00m data\n",
      "File \u001B[0;32m~/PycharmProjects/machine learning/.venv/lib/python3.9/site-packages/urllib3/response.py:949\u001B[0m, in \u001B[0;36mHTTPResponse.read\u001B[0;34m(self, amt, decode_content, cache_content)\u001B[0m\n\u001B[1;32m    946\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_decoded_buffer) \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m amt:\n\u001B[1;32m    947\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_decoded_buffer\u001B[38;5;241m.\u001B[39mget(amt)\n\u001B[0;32m--> 949\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_raw_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43mamt\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    951\u001B[0m flush_decoder \u001B[38;5;241m=\u001B[39m amt \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m (amt \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m data)\n\u001B[1;32m    953\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m data \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_decoded_buffer) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "File \u001B[0;32m~/PycharmProjects/machine learning/.venv/lib/python3.9/site-packages/urllib3/response.py:873\u001B[0m, in \u001B[0;36mHTTPResponse._raw_read\u001B[0;34m(self, amt, read1)\u001B[0m\n\u001B[1;32m    870\u001B[0m fp_closed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fp, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclosed\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m    872\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_error_catcher():\n\u001B[0;32m--> 873\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fp_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43mamt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mread1\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mread1\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m fp_closed \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    874\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m amt \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m amt \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m data:\n\u001B[1;32m    875\u001B[0m         \u001B[38;5;66;03m# Platform-specific: Buggy versions of Python.\u001B[39;00m\n\u001B[1;32m    876\u001B[0m         \u001B[38;5;66;03m# Close the connection when no data is returned\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    881\u001B[0m         \u001B[38;5;66;03m# not properly close the connection in all cases. There is\u001B[39;00m\n\u001B[1;32m    882\u001B[0m         \u001B[38;5;66;03m# no harm in redundantly calling close.\u001B[39;00m\n\u001B[1;32m    883\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fp\u001B[38;5;241m.\u001B[39mclose()\n",
      "File \u001B[0;32m~/PycharmProjects/machine learning/.venv/lib/python3.9/site-packages/urllib3/response.py:856\u001B[0m, in \u001B[0;36mHTTPResponse._fp_read\u001B[0;34m(self, amt, read1)\u001B[0m\n\u001B[1;32m    853\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fp\u001B[38;5;241m.\u001B[39mread1(amt) \u001B[38;5;28;01mif\u001B[39;00m amt \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fp\u001B[38;5;241m.\u001B[39mread1()\n\u001B[1;32m    854\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    855\u001B[0m     \u001B[38;5;66;03m# StringIO doesn't like amt=None\u001B[39;00m\n\u001B[0;32m--> 856\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mamt\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mif\u001B[39;00m amt \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fp\u001B[38;5;241m.\u001B[39mread()\n",
      "File \u001B[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/http/client.py:459\u001B[0m, in \u001B[0;36mHTTPResponse.read\u001B[0;34m(self, amt)\u001B[0m\n\u001B[1;32m    456\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m amt \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    457\u001B[0m     \u001B[38;5;66;03m# Amount is given, implement using readinto\u001B[39;00m\n\u001B[1;32m    458\u001B[0m     b \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mbytearray\u001B[39m(amt)\n\u001B[0;32m--> 459\u001B[0m     n \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreadinto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mb\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    460\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mmemoryview\u001B[39m(b)[:n]\u001B[38;5;241m.\u001B[39mtobytes()\n\u001B[1;32m    461\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    462\u001B[0m     \u001B[38;5;66;03m# Amount is not given (unbounded read) so we must check self.length\u001B[39;00m\n\u001B[1;32m    463\u001B[0m     \u001B[38;5;66;03m# and self.chunked\u001B[39;00m\n",
      "File \u001B[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/http/client.py:503\u001B[0m, in \u001B[0;36mHTTPResponse.readinto\u001B[0;34m(self, b)\u001B[0m\n\u001B[1;32m    498\u001B[0m         b \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mmemoryview\u001B[39m(b)[\u001B[38;5;241m0\u001B[39m:\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlength]\n\u001B[1;32m    500\u001B[0m \u001B[38;5;66;03m# we do not use _safe_read() here because this may be a .will_close\u001B[39;00m\n\u001B[1;32m    501\u001B[0m \u001B[38;5;66;03m# connection, and the user is reading more bytes than will be provided\u001B[39;00m\n\u001B[1;32m    502\u001B[0m \u001B[38;5;66;03m# (for example, reading in 1k chunks)\u001B[39;00m\n\u001B[0;32m--> 503\u001B[0m n \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreadinto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mb\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    504\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m n \u001B[38;5;129;01mand\u001B[39;00m b:\n\u001B[1;32m    505\u001B[0m     \u001B[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001B[39;00m\n\u001B[1;32m    506\u001B[0m     \u001B[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001B[39;00m\n\u001B[1;32m    507\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_close_conn()\n",
      "File \u001B[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/socket.py:704\u001B[0m, in \u001B[0;36mSocketIO.readinto\u001B[0;34m(self, b)\u001B[0m\n\u001B[1;32m    702\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m    703\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 704\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sock\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrecv_into\u001B[49m\u001B[43m(\u001B[49m\u001B[43mb\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    705\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m timeout:\n\u001B[1;32m    706\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_timeout_occurred \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/ssl.py:1241\u001B[0m, in \u001B[0;36mSSLSocket.recv_into\u001B[0;34m(self, buffer, nbytes, flags)\u001B[0m\n\u001B[1;32m   1237\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m flags \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m   1238\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m   1239\u001B[0m           \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m\n\u001B[1;32m   1240\u001B[0m           \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m)\n\u001B[0;32m-> 1241\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnbytes\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbuffer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1242\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1243\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001B[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/ssl.py:1099\u001B[0m, in \u001B[0;36mSSLSocket.read\u001B[0;34m(self, len, buffer)\u001B[0m\n\u001B[1;32m   1097\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1098\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m buffer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m-> 1099\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sslobj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbuffer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1100\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1101\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sslobj\u001B[38;5;241m.\u001B[39mread(\u001B[38;5;28mlen\u001B[39m)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 354
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "get info about table",
   "id": "eabec78eb698c1fd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T09:55:04.353887Z",
     "start_time": "2024-11-28T09:55:04.353795Z"
    }
   },
   "cell_type": "code",
   "source": "df_train.info()",
   "id": "c200494f30722435",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "get info about feature types",
   "id": "ed28cad4c6f8bb1c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_train.dtypes",
   "id": "1eabc1354b795c5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "lets get sample of 15 rows ",
   "id": "273309441fa9c0be"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(df_train.shape)\n",
    "df_train.head()\n",
    "df_train.sample(15)"
   ],
   "id": "9e04d7321a2cddd0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "check for missing values:",
   "id": "34ede96135aaf917"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "df_train.isnull().sum()"
   ],
   "id": "e6bbfbb88d2e455f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "here we are going to delete features where missing ratio of feature is greater than 10%",
   "id": "c4c7bc3bd2822a7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "missing_ratio = df_train.isnull().sum() / len(df_train) * 100\n",
    "print(missing_ratio[missing_ratio > 0].sort_values(ascending=False))\n",
    "\n",
    "threshold = 0.1\n",
    "columns_to_drop = missing_ratio[missing_ratio > threshold * 100].index\n",
    "df_train.drop(columns=columns_to_drop, axis=1, inplace=True)\n",
    "df_test.drop(columns=columns_to_drop, axis=1, inplace=True)\n",
    "print(f\"Ð£Ð´Ð°Ð»ÐµÐ½Ñ‹ ÐºÐ¾Ð»Ð¾Ð½ÐºÐ¸: {columns_to_drop}\")"
   ],
   "id": "4d5373ec85ea0cb6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "here i will delete pointless features in my honest opinion",
   "id": "1aa39bfc6282b860"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "to_exclude= {\"CNT_FAM_MEMBERS\", \"FLAG_DOCUMENT_2\", \"FLAG_DOCUMENT_3\", \"FLAG_DOCUMENT_4\",\n",
    "                    \"FLAG_DOCUMENT_5\", \"FLAG_DOCUMENT_6\", \"FLAG_DOCUMENT_7\", \"FLAG_DOCUMENT_8\", \"FLAG_DOCUMENT_9\",\n",
    "                    \"FLAG_DOCUMENT_10\", \"FLAG_DOCUMENT_11\", \"FLAG_DOCUMENT_12\", \"FLAG_DOCUMENT_13\", \"FLAG_DOCUMENT_14\",\n",
    "                    \"FLAG_DOCUMENT_15\", \"FLAG_DOCUMENT_16\", \"FLAG_DOCUMENT_17\", \"FLAG_DOCUMENT_18\", \"FLAG_DOCUMENT_19\",\n",
    "                    \"FLAG_DOCUMENT_20\", \"FLAG_DOCUMENT_21\", \"REG_REGION_NOT_LIVE_REGION\", \"REGION_POPULATION_RELATIVE\",\n",
    "                    \"HOUR_APPR_PROCESS_START\", \"REG_REGION_NOT_WORK_REGION\", \"LIVE_REGION_NOT_WORK_REGION\",\n",
    "                    \"REG_CITY_NOT_LIVE_CITY\", \"REG_CITY_NOT_WORK_CITY\", \"LIVE_CITY_NOT_WORK_CITY\", \"FLAG_PHONE\",\n",
    "                    \"FLAG_EMAIL\", \"WEEKDAY_APPR_PROCESS_START\", \"ORGANIZATION_TYPE\"}\n",
    "\n",
    "for column in to_exclude:\n",
    "    df_train = df_train.drop(columns=column)\n",
    "    df_test = df_test.drop(columns=column)\n",
    "\n",
    "df_train.sample(15)"
   ],
   "id": "8d58264b6af7071a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "next i will divide categorical and numerical columns",
   "id": "46a22a0e9982994b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "categorical_columns = {\"TARGET\", \"CODE_GENDER\", \"FLAG_OWN_CAR\", \"FLAG_OWN_REALTY\", \"FLAG_MOBIL\", \"FLAG_EMP_PHONE\",\n",
    "                        \"FLAG_WORK_PHONE\", \"FLAG_CONT_MOBILE\", \"REGION_RATING_CLIENT\", \"REGION_RATING_CLIENT_W_CITY\",\n",
    "                        \"NAME_CONTRACT_TYPE\", \"NAME_INCOME_TYPE\", \"NAME_EDUCATION_TYPE\", \"NAME_FAMILY_STATUS\",\n",
    "                        \"NAME_HOUSING_TYPE\"}\n",
    "\n",
    "numerical_columns = [col for col in df_train.columns if col not in categorical_columns]\n",
    "\n",
    "\n",
    "def print_unique():\n",
    "    for feature in categorical_columns:\n",
    "        if feature in df_train.columns:\n",
    "            unique_values = df_train[feature].unique()\n",
    "            print(f\"Unique values for {feature}: {unique_values}\")\n",
    "        else:\n",
    "            print(f\"Feature {feature} not found in the DataFrame.\")\n",
    "\n",
    "\n",
    "print_unique()"
   ],
   "id": "950280e5317f54b3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "lets transform categorical features into numerical formats suitable for machine learning by mapping binary values (e.g., \"Y\"/\"N\" to 1/0) and ordinal values (e.g., education levels) to a defined priority scale. Additionally, it applies one-hot encoding to multi-class categorical variables to create new binary columns for each unique category.",
   "id": "58ba9fb0b72d107b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_train[\"FLAG_OWN_CAR\"] = df_train[\"FLAG_OWN_CAR\"].map({\"N\": 0, \"Y\": 1})\n",
    "df_train[\"FLAG_OWN_REALTY\"] = df_train[\"FLAG_OWN_REALTY\"].map({\"N\": 0, \"Y\": 1})\n",
    "df_train[\"NAME_CONTRACT_TYPE\"] = df_train[\"NAME_CONTRACT_TYPE\"].map({\"Cash loans\": 1, \"Revolving loans\": 0})\n",
    "df_test[\"FLAG_OWN_CAR\"] = df_test[\"FLAG_OWN_CAR\"].map({\"N\": 0, \"Y\": 1})\n",
    "df_test[\"FLAG_OWN_REALTY\"] = df_test[\"FLAG_OWN_REALTY\"].map({\"N\": 0, \"Y\": 1})\n",
    "df_test[\"NAME_CONTRACT_TYPE\"] = df_test[\"NAME_CONTRACT_TYPE\"].map({\"Cash loans\": 1, \"Revolving loans\": 0})\n",
    "\n",
    "education_priority = {\n",
    "    \"Lower secondary\": 0,\n",
    "    \"Secondary / secondary special\": 1,\n",
    "    \"Incomplete higher\": 2,\n",
    "    \"Higher education\": 3,\n",
    "    \"Academic degree\": 4\n",
    "}\n",
    "df_train[\"NAME_EDUCATION_TYPE\"] = df_train[\"NAME_EDUCATION_TYPE\"].map(education_priority)\n",
    "df_test[\"NAME_EDUCATION_TYPE\"] = df_test[\"NAME_EDUCATION_TYPE\"].map(education_priority)\n",
    "\n",
    "df_train = pd.get_dummies(df_train, columns=[\n",
    "    \"NAME_HOUSING_TYPE\", \"NAME_INCOME_TYPE\", \"NAME_EDUCATION_TYPE\", \"NAME_FAMILY_STATUS\"\n",
    "])\n",
    "df_test = pd.get_dummies(df_test, columns=[\n",
    "    \"NAME_HOUSING_TYPE\", \"NAME_INCOME_TYPE\", \"NAME_EDUCATION_TYPE\", \"NAME_FAMILY_STATUS\"\n",
    "])\n",
    "\n",
    "categorical_int_columns = df_train.select_dtypes(include=['bool']).columns"
   ],
   "id": "f47da75854abc8fc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "These functions provide visualization tools for exploring numerical data: histograms display the distribution of features, boxplots reveal outliers and spread, and a heatmap shows correlations among variables in a dataset.",
   "id": "566ba6170f9f2fa1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def plot_numerical_features_histogram(data, numerical_features, bins=30, figsize=(16, 20), color='g', xlabelsize=8,\n",
    "                                      ylabelsize=8, title=\"Histograms of Numerical Features\"):\n",
    "    fig = data[numerical_features].hist(\n",
    "        figsize=figsize,\n",
    "        color=color,\n",
    "        bins=bins,\n",
    "        xlabelsize=xlabelsize,\n",
    "        ylabelsize=ylabelsize\n",
    "    )\n",
    "\n",
    "    plt.suptitle(title, fontsize=16, y=1.02)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_boxplots(data, numerical_features, cols=2, figsize=(10, 4)):\n",
    "    num_features = len(numerical_features)\n",
    "    rows = math.ceil(num_features / cols)\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=rows, ncols=cols, figsize=(figsize[0] * cols, figsize[1] * rows))\n",
    "    fig.tight_layout(pad=3.0)\n",
    "\n",
    "    axes = axes.flatten() if num_features > 1 else [axes]\n",
    "\n",
    "    for i, col in enumerate(numerical_features):\n",
    "        sns.boxplot(x=data[col], ax=axes[i], color='skyblue')\n",
    "        axes[i].set_title(f'Boxplot for {col}', fontsize=10)\n",
    "        axes[i].set_xlabel(col, fontsize=8)\n",
    "        axes[i].set_ylabel('Value', fontsize=8)\n",
    "\n",
    "    for j in range(len(numerical_features), len(axes)):\n",
    "        axes[j].axis('off')\n",
    "\n",
    "    plt.suptitle(\"Boxplots of Numerical Features\", fontsize=16, y=1.02)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_correlation_heatmap(data, figsize=(16, 12), cmap=\"Greens\", vmax=0.8, annot=True):\n",
    "    corr_matrix = data.corr()\n",
    "\n",
    "    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    fig.set_size_inches(20, 15)\n",
    "    sns.heatmap(\n",
    "        corr_matrix,\n",
    "        mask=mask,\n",
    "        cmap=cmap,\n",
    "        vmax=vmax,\n",
    "        square=True,\n",
    "        annot=annot,\n",
    "        fmt=\".2f\",\n",
    "        linewidths=0.5,\n",
    "        cbar_kws={\"shrink\": 0.8},\n",
    "        ax=ax\n",
    "    )\n",
    "\n",
    "    plt.title(\"Correlation Heatmap\", fontsize=18, pad=20)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "id": "ac0fa0f18edb0e38",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "lets get histogram",
   "id": "d0ee6564f69195cd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plot_numerical_features_histogram(df_train, numerical_columns, bins=20, color='skyblue',\n",
    "                                  title=\"Numerical Feature Distributions\")"
   ],
   "id": "5a79a945dfb65698",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "lets extract and lists the names of all numerical columns",
   "id": "34dd1a6921c690d2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "7f7e87e863a632c4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "numerical_features = df_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "print(numerical_features)"
   ],
   "id": "a089676175ae6bc0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "correlation heatmap:",
   "id": "488636aa26a84bd8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_correlation_heatmap(df_train[numerical_features])",
   "id": "8dbae0a957192883",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "then i will exclude columns which correlate the most",
   "id": "28e461775ea65e86"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "to_exclude = {\"AMT_ANNUITY\", \"AMT_GOODS_PRICE\", \"OBS_60_CNT_SOCIAL_CIRCLE\", \"DEF_60_CNT_SOCIAL_CIRCLE\",\n",
    "              \"NAME_INCOME_TYPE_Pensioner\"}\n",
    "df_train = df_train.drop(columns=to_exclude)\n",
    "df_test = df_test.drop(columns=to_exclude)"
   ],
   "id": "2f7b0e11dc2051eb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "lets see updated list of numerical features ",
   "id": "99de20d8ae91a6b7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "numerical_features = df_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "print(numerical_features)"
   ],
   "id": "f1e00786f4e83c85",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "here we clean time-related features, compute a social circle median, drop unused columns, and plots numerical features with boxplots.",
   "id": "c8459f40580d0970"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#OUTLIERS\n",
    "\n",
    "df_train['DAYS_EMPLOYED'] = df_train['DAYS_EMPLOYED'].replace(365243, 0).abs()\n",
    "df_train['DAYS_BIRTH'] = df_train['DAYS_BIRTH'].abs()\n",
    "df_train['DAYS_REGISTRATION'] = df_train['DAYS_REGISTRATION'].abs()\n",
    "df_train['DAYS_ID_PUBLISH'] = df_train['DAYS_ID_PUBLISH'].abs()\n",
    "df_test['DAYS_EMPLOYED'] = df_test['DAYS_EMPLOYED'].replace(365243, 0).abs()\n",
    "df_test['DAYS_BIRTH'] = df_test['DAYS_BIRTH'].abs()\n",
    "df_test['DAYS_REGISTRATION'] = df_test['DAYS_REGISTRATION'].abs()\n",
    "df_test['DAYS_ID_PUBLISH'] = df_test['DAYS_ID_PUBLISH'].abs()\n",
    "\n",
    "df_test['SOCIAL_CIRCLE_MEDIAN'] = df_test[['OBS_30_CNT_SOCIAL_CIRCLE', 'DEF_30_CNT_SOCIAL_CIRCLE']].median(axis=1)\n",
    "df_train['SOCIAL_CIRCLE_MEDIAN'] = df_train[['OBS_30_CNT_SOCIAL_CIRCLE', 'DEF_30_CNT_SOCIAL_CIRCLE']].median(axis=1)\n",
    "df_train.drop(['OBS_30_CNT_SOCIAL_CIRCLE', 'DEF_30_CNT_SOCIAL_CIRCLE'], axis=1, inplace=True)\n",
    "df_test.drop(['OBS_30_CNT_SOCIAL_CIRCLE', 'DEF_30_CNT_SOCIAL_CIRCLE'], axis=1, inplace=True)\n",
    "numerical_features = {\"CNT_CHILDREN\", \"AMT_INCOME_TOTAL\", \"DAYS_BIRTH\", \"AMT_CREDIT\", \"DAYS_EMPLOYED\",\n",
    "                      \"DAYS_REGISTRATION\", \"EXT_SOURCE_2\", \"DAYS_ID_PUBLISH\"}\n",
    "plot_boxplots(df_train, numerical_features)"
   ],
   "id": "6eaf2fd57faeb73c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Here we prepare the data by splitting it into features and target, handle missing values, apply preprocessing for numerical and categorical columns, build a model pipeline with logistic regression, train the model, and evaluate its performance using ROC AUC.",
   "id": "2cc0e018964fcc4c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "target_column = 'TARGET'\n",
    "X = df_train.drop(columns=[target_column])\n",
    "y = df_train[target_column]\n",
    "\n",
    "numerical_columns = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_columns = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "X[numerical_columns] = X[numerical_columns].fillna(X[numerical_columns].median())\n",
    "X[categorical_columns] = X[categorical_columns].fillna(X[categorical_columns].mode().iloc[0])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='median')), \n",
    "            ('scaler', StandardScaler())\n",
    "        ]), numerical_columns),\n",
    "\n",
    "        ('cat', Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "            ('onehot', OneHotEncoder(drop='first', handle_unknown='ignore'))\n",
    "        ]), categorical_columns)\n",
    "    ])\n",
    "\n",
    "model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(max_iter=200, solver='lbfgs', random_state=42))\n",
    "])\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "model.fit(train_x, train_y)\n",
    "\n",
    "test_preds_proba = model.predict_proba(test_x)[:, 1]\n",
    "roc_auc = roc_auc_score(test_y, test_preds_proba)\n",
    "\n",
    "fpr, tpr, _ = roc_curve(test_y, test_preds_proba)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, label=f\"ROC curve (AUC = {roc_auc:.4f})\")\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label=\"Random guess\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "print(f\"ROC AUC Score: {roc_auc:.4f}\")"
   ],
   "id": "c11c0c9a6b25d2f2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "lets get result file",
   "id": "5b25ac6347943cd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "scaled_test_x = model.named_steps['preprocessor'].transform(df_test)\n",
    "predicted_probabilities = model.named_steps['classifier'].predict_proba(scaled_test_x)[:, 1]\n",
    "\n",
    "submission_df = pd.DataFrame({\n",
    "    \"SK_ID_CURR\": df_test['SK_ID_CURR'],\n",
    "    \"TARGET\": predicted_probabilities\n",
    "}).to_csv"
   ],
   "id": "52fab92040036636",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "result: https://imgur.com/a/ISljz9s",
   "id": "5ee0d4129455c6f0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
